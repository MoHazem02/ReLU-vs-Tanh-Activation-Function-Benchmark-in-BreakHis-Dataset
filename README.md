# ğŸ§¬ Activation Functions in CNNs â€” A Comparative Study (ReLU vs Tanh)

This project compares the performance of **ReLU** and **Tanh** activation functions in a **Convolutional Neural Network (CNN)** designed to classify **breast histopathology images** from the **BreakHis dataset** into *benign* or *malignant* categories.

---


<img width="1160" height="504" alt="Untitled" src="https://github.com/user-attachments/assets/93597051-db5b-4e03-a0db-452eeee17f92" />


## ğŸ“ Project Overview

This study aims to explore how the choice of activation function affects a CNNâ€™s **training behavior**, **accuracy**, and **convergence** in a real-world medical imaging dataset.

Two models were built using identical architectures â€” differing only in their hidden-layer activations:
- Model 1 â†’ `ReLU`
- Model 2 â†’ `Tanh`

Both models used:
- **Sigmoid** activation in the output layer (binary classification)
- **Binary cross-entropy** loss
- **Adam optimizer**

---

## ğŸ§© Model Architecture

| Layer Type | Filters / Units | Activation | Notes |
|-------------|----------------|-------------|--------|
| Conv2D | 32 | ReLU / Tanh | 3Ã—3 kernel, same padding |
| MaxPooling2D | â€” | â€” | 2Ã—2 pool size |
| Conv2D | 64 | ReLU / Tanh | 3Ã—3 kernel |
| MaxPooling2D | â€” | â€” | 2Ã—2 pool size |
| Conv2D | 128 | ReLU / Tanh | 3Ã—3 kernel |
| MaxPooling2D | â€” | â€” | 2Ã—2 pool size |
| Flatten | â€” | â€” | â€” |
| Dense | 128 | ReLU / Tanh | Fully connected |
| Dense | 64 | ReLU / Tanh | Fully connected |
| Dense | 1 | Sigmoid | Output layer |

---

## âš™ï¸ Training Details

| Parameter | Value |
|------------|--------|
| Image Size | 96Ã—96Ã—3 |
| Epochs | 10 |
| Batch Size | 32 |
| Optimizer | Adam (lr = 0.001) |
| Loss Function | Binary Crossentropy |
| Validation Split | 20% |

---

## ğŸ“Š Results

| Metric | ReLU | Tanh |
|---------|------|------|
| **Validation Accuracy** | ~86% | ~69% |
| **Test Accuracy** | **0.867** | **0.686** |
| **Observation** | ReLU achieved faster convergence and higher accuracy compared to Tanh, which struggled due to vanishing gradients. |

---

## ğŸ§© Key Insights

- **ReLU outperforms Tanh** in deeper CNNs due to reduced vanishing gradient effects.  
- Tanh can still be effective for shallow networks or small datasets but saturates faster.  
- Proper normalization and balanced datasets are crucial for fair activation comparisons.

---

## ğŸš€ Technologies Used

- **Python 3**
- **TensorFlow / Keras**
- **OpenCV**
- **NumPy**
- **Matplotlib**
- **tqdm**

---

<img width="790" height="590" alt="Untitled" src="https://github.com/user-attachments/assets/f63f4f76-0a3d-4c16-b9ab-edf3b8ef0b8e" />


## ğŸ§  Future Work

- Extend to additional activation functions (LeakyReLU, ELU, Swish)
- Apply model interpretability (Grad-CAM)
- Evaluate transfer learning on BreakHis (using VGG16 / ResNet)

---

## ğŸ‘¨â€ğŸ’» Author

**Mohamed Hazem**  
Biomedical and Data Engineering Student  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/mohamed-hazem01/) &nbsp;&nbsp; ğŸ’» [GitHub](https://github.com/MoHazem02)

---

## ğŸ Conclusion

This project demonstrates how a **single design choice â€” activation function â€”** can significantly influence CNN learning dynamics, stability, and performance, especially in **medical image classification** tasks.

---

â­ **If you found this project helpful, consider giving it a star!**
